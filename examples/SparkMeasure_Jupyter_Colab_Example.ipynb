{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hekowIzpjNSY"
      },
      "source": [
        "# Jupyter/Colab Notebook to Showcase sparkMeasure APIs for Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAvFWHY2jNSf"
      },
      "source": [
        "### [Run on Google Colab Research: <img src=\"https://raw.githubusercontent.com/googlecolab/open_in_colab/master/images/icon128.png\">](https://colab.research.google.com/github/LucaCanali/sparkMeasure/blob/master/examples/SparkMeasure_Jupyter_Colab_Example.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXxWMkJDjNSh"
      },
      "source": [
        "**SparkMeasure is a tool for performance troubleshooting of Apache Spark workloads**  \n",
        "It simplifies the collection and analysis of Spark performance metrics. It is also intended as a working example of how to use Spark listeners for collecting and processing Spark executors task metrics data.\n",
        "\n",
        "**References:**\n",
        "- [https://github.com/LucaCanali/sparkMeasure](https://github.com/LucaCanali/sparkMeasure)  \n",
        "- sparkmeasure Python docs: [docs/Python_shell_and_Jupyter](https://github.com/LucaCanali/sparkMeasure/blob/master/docs/Python_shell_and_Jupyter.md)  \n",
        "\n",
        "**Architecture:**\n",
        "![sparkMeasure architecture diagram](https://github.com/LucaCanali/sparkMeasure/raw/master/docs/sparkMeasure_architecture_diagram.png)\n",
        "\n",
        "Contact: Luca.Canali@cern.ch, February 2019  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB1boA7GjNSk"
      },
      "outputs": [],
      "source": [
        "# Install Spark \n",
        "# Note: This installs the Spark version 2.4.3 on newer does not work.\n",
        "\n",
        "!pip install pyspark==2.4.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2Wxld1HejNSm"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark Session\n",
        "# This example uses a local cluster, you can modify master to use  YARN or K8S if available \n",
        "# This example downloads sparkMeasure 0.14 for scala 2_11 from maven central\n",
        "\n",
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .master(\"local[*]\") \\\n",
        " .appName(\"Test sparkmeasure instrumentation of Python/PySpark code\") \\\n",
        " .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.11:0.14\")  \\\n",
        " .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "P_j6Lw6rjNSn",
        "outputId": "8495a407-35ff-4066-910e-5f19ffa76a6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+\n",
            "| id|    Greeting|\n",
            "+---+------------+\n",
            "|  1|Hello world!|\n",
            "+---+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# test that Spark is working OK\n",
        "spark.sql(\"select 1 as id, 'Hello world!' as Greeting\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96j_mM9qjNSo"
      },
      "outputs": [],
      "source": [
        "# Install the Python wrapper API for spark-measure\n",
        "\n",
        "!pip install sparkmeasure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TjoadZqTjNSq"
      },
      "outputs": [],
      "source": [
        "# Load the Python API in sparkmeasure package\n",
        "# an attache the sparkMeasure Listener for stagemetrics to the active Spark session\n",
        "\n",
        "from sparkmeasure import StageMetrics\n",
        "stagemetrics = StageMetrics(spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GfDo7JaujNSs"
      },
      "outputs": [],
      "source": [
        "# Define cell and line magic to wrap the instrumentation\n",
        "from IPython.core.magic import (register_line_magic, register_cell_magic, register_line_cell_magic)\n",
        "\n",
        "@register_line_cell_magic\n",
        "def sparkmeasure(line, cell=None):\n",
        "    \"run and measure spark workload. Use: %sparkmeasure or %%sparkmeasure\"\n",
        "    val = cell if cell is not None else line\n",
        "    stagemetrics.begin()\n",
        "    eval(val)\n",
        "    stagemetrics.end()\n",
        "    stagemetrics.print_report()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", '-1')"
      ],
      "metadata": {
        "id": "MTEqgG8HmNJS"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "queCqrXajNSt",
        "outputId": "64ae3879-e87f-49ad-cfef-ded19fbd59d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "| count(1)|\n",
            "+---------+\n",
            "|100000000|\n",
            "+---------+\n",
            "\n",
            "\n",
            "Scheduling mode = FIFO\n",
            "Spark Context default degree of parallelism = 2\n",
            "Aggregated Spark stage metrics:\n",
            "numStages => 2\n",
            "sum(numTasks) => 9\n",
            "elapsedTime => 9209 (9 s)\n",
            "sum(stageDuration) => 9208 (9 s)\n",
            "sum(executorRunTime) => 18066 (18 s)\n",
            "sum(executorCpuTime) => 11305 (11 s)\n",
            "sum(executorDeserializeTime) => 38 (38 ms)\n",
            "sum(executorDeserializeCpuTime) => 17 (17 ms)\n",
            "sum(resultSerializationTime) => 1 (1 ms)\n",
            "sum(jvmGCTime) => 171 (0.2 s)\n",
            "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
            "sum(shuffleWriteTime) => 12 (12 ms)\n",
            "max(resultSize) => 16891 (16.0 KB)\n",
            "sum(numUpdatedBlockStatuses) => 0\n",
            "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
            "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
            "max(peakExecutionMemory) => 0\n",
            "sum(recordsRead) => 8400\n",
            "sum(bytesRead) => 0 (0 Bytes)\n",
            "sum(recordsWritten) => 0\n",
            "sum(bytesWritten) => 0 (0 Bytes)\n",
            "sum(shuffleTotalBytesRead) => 472 (472 Bytes)\n",
            "sum(shuffleTotalBlocksFetched) => 8\n",
            "sum(shuffleLocalBlocksFetched) => 8\n",
            "sum(shuffleRemoteBlocksFetched) => 0\n",
            "sum(shuffleBytesWritten) => 472 (472 Bytes)\n",
            "sum(shuffleRecordsWritten) => 8\n"
          ]
        }
      ],
      "source": [
        "%%sparkmeasure\n",
        "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "SsimhBofjNSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df604d0d-3502-41b9-9126-19aebceab71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "| count(1)|\n",
            "+---------+\n",
            "|100000000|\n",
            "+---------+\n",
            "\n",
            "\n",
            "Scheduling mode = FIFO\n",
            "Spark Context default degree of parallelism = 2\n",
            "Aggregated Spark stage metrics:\n",
            "numStages => 2\n",
            "sum(numTasks) => 9\n",
            "elapsedTime => 8833 (9 s)\n",
            "sum(stageDuration) => 8832 (9 s)\n",
            "sum(executorRunTime) => 17217 (17 s)\n",
            "sum(executorCpuTime) => 11414 (11 s)\n",
            "sum(executorDeserializeTime) => 39 (39 ms)\n",
            "sum(executorDeserializeCpuTime) => 16 (16 ms)\n",
            "sum(resultSerializationTime) => 1 (1 ms)\n",
            "sum(jvmGCTime) => 128 (0.1 s)\n",
            "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
            "sum(shuffleWriteTime) => 12 (12 ms)\n",
            "max(resultSize) => 16891 (16.0 KB)\n",
            "sum(numUpdatedBlockStatuses) => 0\n",
            "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
            "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
            "max(peakExecutionMemory) => 0\n",
            "sum(recordsRead) => 8400\n",
            "sum(bytesRead) => 0 (0 Bytes)\n",
            "sum(recordsWritten) => 0\n",
            "sum(bytesWritten) => 0 (0 Bytes)\n",
            "sum(shuffleTotalBytesRead) => 472 (472 Bytes)\n",
            "sum(shuffleTotalBlocksFetched) => 8\n",
            "sum(shuffleLocalBlocksFetched) => 8\n",
            "sum(shuffleRemoteBlocksFetched) => 0\n",
            "sum(shuffleBytesWritten) => 472 (472 Bytes)\n",
            "sum(shuffleRecordsWritten) => 8\n"
          ]
        }
      ],
      "source": [
        "# You can also explicitly Wrap your Spark workload into stagemetrics instrumentation \n",
        "# as in this example\n",
        "stagemetrics.begin()\n",
        "\n",
        "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show()\n",
        "\n",
        "stagemetrics.end()\n",
        "# Print a summary report\n",
        "stagemetrics.print_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "921F8MDNjNSw",
        "outputId": "71ca9164-34e0-4655-814a-ed992c9a3f10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "| count(1)|\n",
            "+---------+\n",
            "|100000000|\n",
            "+---------+\n",
            "\n",
            "\n",
            "Scheduling mode = FIFO\n",
            "Spark Context default degree of parallelism = 2\n",
            "Aggregated Spark stage metrics:\n",
            "numStages => 2\n",
            "sum(numTasks) => 9\n",
            "elapsedTime => 6323 (6 s)\n",
            "sum(stageDuration) => 6322 (6 s)\n",
            "sum(executorRunTime) => 12518 (13 s)\n",
            "sum(executorCpuTime) => 11666 (12 s)\n",
            "sum(executorDeserializeTime) => 31 (31 ms)\n",
            "sum(executorDeserializeCpuTime) => 15 (15 ms)\n",
            "sum(resultSerializationTime) => 2 (2 ms)\n",
            "sum(jvmGCTime) => 86 (86 ms)\n",
            "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
            "sum(shuffleWriteTime) => 3 (3 ms)\n",
            "max(resultSize) => 16891 (16.0 KB)\n",
            "sum(numUpdatedBlockStatuses) => 0\n",
            "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
            "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
            "max(peakExecutionMemory) => 0\n",
            "sum(recordsRead) => 8400\n",
            "sum(bytesRead) => 0 (0 Bytes)\n",
            "sum(recordsWritten) => 0\n",
            "sum(bytesWritten) => 0 (0 Bytes)\n",
            "sum(shuffleTotalBytesRead) => 472 (472 Bytes)\n",
            "sum(shuffleTotalBlocksFetched) => 8\n",
            "sum(shuffleLocalBlocksFetched) => 8\n",
            "sum(shuffleRemoteBlocksFetched) => 0\n",
            "sum(shuffleBytesWritten) => 472 (472 Bytes)\n",
            "sum(shuffleRecordsWritten) => 8\n"
          ]
        }
      ],
      "source": [
        "# Another way to encapsulate code and instrumentation in a compact form\n",
        "\n",
        "result = stagemetrics.runandmeasure(locals(), \"\"\"\n",
        "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show()\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWQ3cC_OjNSx"
      },
      "source": [
        "## Example of collecting using Task Metrics\n",
        "Collecting Spark task metrics at the granularity of each task completion has additional overhead\n",
        "compare to collecting at the stage completion level, therefore this option should only be used if you need data with this finer granularity, for example because you want\n",
        "to study skew effects, otherwise consider using stagemetrics aggregation as preferred choice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-6IN80W_jNSx",
        "outputId": "73fdefc7-c35d-40e7-f903-88dc59bf2dac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "| count(1)|\n",
            "+---------+\n",
            "|100000000|\n",
            "+---------+\n",
            "\n",
            "\n",
            "Scheduling mode = FIFO\n",
            "Spark Contex default degree of parallelism = 2\n",
            "Aggregated Spark task metrics:\n",
            "numtasks => 9\n",
            "elapsedTime => 6211 (6 s)\n",
            "sum(duration) => 12354 (12 s)\n",
            "sum(schedulerDelay) => 28\n",
            "sum(executorRunTime) => 12304 (12 s)\n",
            "sum(executorCpuTime) => 11639 (12 s)\n",
            "sum(executorDeserializeTime) => 22 (22 ms)\n",
            "sum(executorDeserializeCpuTime) => 9 (9 ms)\n",
            "sum(resultSerializationTime) => 0 (0 ms)\n",
            "sum(jvmGCTime) => 94 (94 ms)\n",
            "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
            "sum(shuffleWriteTime) => 0 (0 ms)\n",
            "sum(gettingResultTime) => 0 (0 ms)\n",
            "max(resultSize) => 2106 (2.0 KB)\n",
            "sum(numUpdatedBlockStatuses) => 0\n",
            "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
            "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
            "max(peakExecutionMemory) => 0\n",
            "sum(recordsRead) => 8400\n",
            "sum(bytesRead) => 0 (0 Bytes)\n",
            "sum(recordsWritten) => 0\n",
            "sum(bytesWritten) => 0 (0 Bytes)\n",
            "sum(shuffleTotalBytesRead) => 472 (472 Bytes)\n",
            "sum(shuffleTotalBlocksFetched) => 8\n",
            "sum(shuffleLocalBlocksFetched) => 8\n",
            "sum(shuffleRemoteBlocksFetched) => 0\n",
            "sum(shuffleBytesWritten) => 472 (472 Bytes)\n",
            "sum(shuffleRecordsWritten) => 8\n"
          ]
        }
      ],
      "source": [
        "from sparkmeasure import TaskMetrics\n",
        "taskmetrics = TaskMetrics(spark)\n",
        "\n",
        "taskmetrics.begin()\n",
        "spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show()\n",
        "taskmetrics.end()\n",
        "taskmetrics.print_report()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report = taskmetrics.report()"
      ],
      "metadata": {
        "id": "T45pmHahqR8u"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(report)"
      ],
      "metadata": {
        "id": "iQS49EY-rCMl",
        "outputId": "7c3aeae6-4a03-41a3-e1c1-d51388d00e0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scheduling mode = FIFO\n",
            "Spark Contex default degree of parallelism = 2\n",
            "Aggregated Spark task metrics:\n",
            "numtasks => 9\n",
            "elapsedTime => 6211 (6 s)\n",
            "sum(duration) => 12354 (12 s)\n",
            "sum(schedulerDelay) => 28\n",
            "sum(executorRunTime) => 12304 (12 s)\n",
            "sum(executorCpuTime) => 11639 (12 s)\n",
            "sum(executorDeserializeTime) => 22 (22 ms)\n",
            "sum(executorDeserializeCpuTime) => 9 (9 ms)\n",
            "sum(resultSerializationTime) => 0 (0 ms)\n",
            "sum(jvmGCTime) => 94 (94 ms)\n",
            "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
            "sum(shuffleWriteTime) => 0 (0 ms)\n",
            "sum(gettingResultTime) => 0 (0 ms)\n",
            "max(resultSize) => 2106 (2.0 KB)\n",
            "sum(numUpdatedBlockStatuses) => 0\n",
            "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
            "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
            "max(peakExecutionMemory) => 0\n",
            "sum(recordsRead) => 8400\n",
            "sum(bytesRead) => 0 (0 Bytes)\n",
            "sum(recordsWritten) => 0\n",
            "sum(bytesWritten) => 0 (0 Bytes)\n",
            "sum(shuffleTotalBytesRead) => 472 (472 Bytes)\n",
            "sum(shuffleTotalBlocksFetched) => 8\n",
            "sum(shuffleLocalBlocksFetched) => 8\n",
            "sum(shuffleRemoteBlocksFetched) => 0\n",
            "sum(shuffleBytesWritten) => 472 (472 Bytes)\n",
            "sum(shuffleRecordsWritten) => 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "nr2IO345jNSy"
      },
      "outputs": [],
      "source": [
        "def measure(my_job):\n",
        "  taskmetrics.begin()\n",
        "  my_job()\n",
        "  taskmetrics.end()\n",
        "  return taskmetrics.report()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_report = measure(spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show)"
      ],
      "metadata": {
        "id": "ago7KV9ormns",
        "outputId": "4385d91b-2da4-4e59-ea7e-97846f5ff399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "| count(1)|\n",
            "+---------+\n",
            "|100000000|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_report)"
      ],
      "metadata": {
        "id": "YIsYtpGVrqnM",
        "outputId": "7cc1fb3b-69c7-45fe-b46e-95352cfa82a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Scheduling mode = FIFO\n",
            "Spark Contex default degree of parallelism = 2\n",
            "Aggregated Spark task metrics:\n",
            "numtasks => 9\n",
            "elapsedTime => 6294 (6 s)\n",
            "sum(duration) => 12522 (13 s)\n",
            "sum(schedulerDelay) => 36\n",
            "sum(executorRunTime) => 12462 (12 s)\n",
            "sum(executorCpuTime) => 11628 (12 s)\n",
            "sum(executorDeserializeTime) => 21 (21 ms)\n",
            "sum(executorDeserializeCpuTime) => 9 (9 ms)\n",
            "sum(resultSerializationTime) => 3 (3 ms)\n",
            "sum(jvmGCTime) => 150 (0.2 s)\n",
            "sum(shuffleFetchWaitTime) => 0 (0 ms)\n",
            "sum(shuffleWriteTime) => 0 (0 ms)\n",
            "sum(gettingResultTime) => 0 (0 ms)\n",
            "max(resultSize) => 2149 (2.0 KB)\n",
            "sum(numUpdatedBlockStatuses) => 0\n",
            "sum(diskBytesSpilled) => 0 (0 Bytes)\n",
            "sum(memoryBytesSpilled) => 0 (0 Bytes)\n",
            "max(peakExecutionMemory) => 0\n",
            "sum(recordsRead) => 8400\n",
            "sum(bytesRead) => 0 (0 Bytes)\n",
            "sum(recordsWritten) => 0\n",
            "sum(bytesWritten) => 0 (0 Bytes)\n",
            "sum(shuffleTotalBytesRead) => 472 (472 Bytes)\n",
            "sum(shuffleTotalBlocksFetched) => 8\n",
            "sum(shuffleLocalBlocksFetched) => 8\n",
            "sum(shuffleRemoteBlocksFetched) => 0\n",
            "sum(shuffleBytesWritten) => 472 (472 Bytes)\n",
            "sum(shuffleRecordsWritten) => 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparkmeasure import TaskMetrics\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        " .builder \\\n",
        " .master(\"local[*]\") \\\n",
        " .appName(\"Test sparkmeasure instrumentation of Python/PySpark code\") \\\n",
        " .config(\"spark.jars.packages\",\"ch.cern.sparkmeasure:spark-measure_2.11:0.14\")  \\\n",
        " .getOrCreate()\n",
        "\n",
        "def measure(my_job):\n",
        "  taskmetrics = TaskMetrics(spark)\n",
        "  taskmetrics.begin()\n",
        "  my_job()\n",
        "  taskmetrics.end()\n",
        "  return taskmetrics.report()"
      ],
      "metadata": {
        "id": "cQYs9w9ArtCF"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "measure(spark.sql(\"select count(*) from range(1000) cross join range(1000) cross join range(100)\").show)"
      ],
      "metadata": {
        "id": "tfN9qXGlsXOM",
        "outputId": "02aed9dd-fef2-42f2-d769-7132e7c422e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "| count(1)|\n",
            "+---------+\n",
            "|100000000|\n",
            "+---------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nScheduling mode = FIFO\\nSpark Contex default degree of parallelism = 2\\nAggregated Spark task metrics:\\nnumtasks => 9\\nelapsedTime => 6292 (6 s)\\nsum(duration) => 12559 (13 s)\\nsum(schedulerDelay) => 30\\nsum(executorRunTime) => 12505 (13 s)\\nsum(executorCpuTime) => 11654 (12 s)\\nsum(executorDeserializeTime) => 24 (24 ms)\\nsum(executorDeserializeCpuTime) => 8 (8 ms)\\nsum(resultSerializationTime) => 0 (0 ms)\\nsum(jvmGCTime) => 122 (0.1 s)\\nsum(shuffleFetchWaitTime) => 0 (0 ms)\\nsum(shuffleWriteTime) => 0 (0 ms)\\nsum(gettingResultTime) => 0 (0 ms)\\nmax(resultSize) => 2106 (2.0 KB)\\nsum(numUpdatedBlockStatuses) => 0\\nsum(diskBytesSpilled) => 0 (0 Bytes)\\nsum(memoryBytesSpilled) => 0 (0 Bytes)\\nmax(peakExecutionMemory) => 0\\nsum(recordsRead) => 8400\\nsum(bytesRead) => 0 (0 Bytes)\\nsum(recordsWritten) => 0\\nsum(bytesWritten) => 0 (0 Bytes)\\nsum(shuffleTotalBytesRead) => 472 (472 Bytes)\\nsum(shuffleTotalBlocksFetched) => 8\\nsum(shuffleLocalBlocksFetched) => 8\\nsum(shuffleRemoteBlocksFetched) => 0\\nsum(shuffleBytesWritten) => 472 (472 Bytes)\\nsum(shuffleRecordsWritten) => 8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "SparkMeasure_Jupyter_Colab_Example (1).ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}